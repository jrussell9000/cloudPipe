apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  namespace: argo-workflows
  #name: func-postproc-master-workflow-template
  generateName: func-postproc-master-workflow-
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::575108944090:role/argo-workflows-runner

spec:
  # Pre-specify metadata for all workflows run from this workflow
  workflowMetadata:
    annotations:
      workflows.argoproj.io/title: "sub-{{workflow.parameters.ndaGUID}}"
    labelsFrom:
      subjectID: 
        expression: workflow.parameters.ndaGUID
  podMetadata:
    labels:
      subjectid: "sub-{{workflow.parameters.ndaGUID}}"

  # Create the following persistent volume claim for this workflow
  volumeClaimTemplates:
    # Naming the volume claim after the subject ID (for tracking)
    - metadata:
        # "Name" should be used to reference this PVC further down the workflow
        name: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
        # Specify a string that will be appended to the string created by generateName in the workflow metadata
        # By introducing the random character sequence from the metadata, we create a unique PVC for each workflow
        # run, even if it's for the same subject ID. Neat!
        generateName: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
      spec:
        accessModes:
          - ReadWriteMany
        storageClassName: efs-sc
        resources:
          requests:
            storage: 30Gi

  # When to clean up persistent volume claims
  volumeClaimGC: 
    strategy: OnWorkflowCompletion

  serviceAccountName: argo-workflows-runner

  # Time to live after workflow is completed, replaces ttlSecondsAfterFinished
  ttlStrategy:
    secondsAfterCompletion: 360 

  # When should an individual workflow step be retried and how many times max?
  retryStrategy:
    retryPolicy: OnError
    limit: '1'

  podDisruptionBudget:
    minAvailable: 9999

  # MUST include a global security context to ensure artifacts are passed with the same (accessible) 
  # permissions or else some tasks may not be able to access passed artifacts (this took too long to figure out)
  # see: https://github.com/argoproj/argo-workflows/issues/9651 (can also set 'mode: 755' e.g., to define permissions)
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  arguments:
    parameters:
      # Workflow parameters passed from command-line (-p) are indicated by lack of an accompanying value
      - name: ndaGUID
      # - name: miNDARpkgID
      # Creating new workflow parameters by modifying existing ones (using expr language)
      - name: subjID
        # Prepending the NDA GUID with 'sub-' per BIDS format
        value: sub-{{workflow.parameters.ndaGUID}}

  # Which template (below) should we start with?
  entrypoint: master-pipeline-dag

  templates:

  - name: master-pipeline-dag
    dag:
      tasks:
        - name: functional-session-inventory-dagtask
          template: functional-session-inventory-template

        - name: functional-session-processing-dagtask
          template: functional-session-processing-dag-template
          arguments:
            parameters:
              - name: session
                value: "{{item}}"
          withParam: "{{tasks.functional-session-inventory-dagtask.outputs.result}}"
          dependencies:
            - functional-session-inventory-dagtask


  - name: functional-session-processing-dag-template
    inputs:
      parameters:
        - name: session

    dag:
      tasks:
        - name: functional-task-inventory-dagtask
          template: functional-task-inventory-template
          arguments:
            parameters:
              - name: session
                value: "{{inputs.parameters.session}}"

        - name: functional-task-processing-dagtask
          template: functional-task-processing-template
          arguments:
            parameters:
              - name: session
                value: "{{inputs.parameters.session}}"
              - name: task
                value: "{{item}}"
          withParam: "{{tasks.functional-task-inventory-dagtask.outputs.result}}"
          dependencies:
            - functional-task-inventory-dagtask


  - name: functional-session-inventory-template
    securityContext:
      runAsUser: 0
    script:
      image: public.ecr.aws/docker/library/python:3-slim-bullseye
      imagePullPolicy: IfNotPresent
      resources:
        requests:
          ephemeral-storage: 50M
      command: [python]
      source: |
        from pathlib import Path
        import json
        import os
        import time

        try:
          import boto3
        except ImportError:
          os.system('python -m pip -q install --root-user-action=ignore boto3')
          import boto3

        s3 = boto3.client('s3')
        sessions = []
        response = s3.list_objects_v2(
          Bucket='abcd-working',
          Prefix='derivatives/fmriprep/{{workflow.parameters.subjID}}/',
          Delimiter='/')
        for o in response.get('CommonPrefixes'):
          prefix = o.get('Prefix')
          session = prefix.split('/')[3]
          sessions.append(session)
        print(json.dumps(sessions))


  - name: functional-task-inventory-template
    inputs:
      parameters:
        - name: session
      artifacts:
        - name: funcpreproc
          path: "/home/fmriprep/output/{{workflow.parameters.subjID}}"
          s3:
            key: 'derivatives/fmriprep/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/fmriprep-output.tar.gz'
        - name: dataset-description
          path: /home/fmriprep/output/dataset_description.json
          s3:
            key: 'config/dataset_description.json'
    script:
      volumeMounts:
        # fmriprep base directory
        # PersistentVolumeClaim reference
      - name: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
        # Path inside the container where the volume will be mounted
        mountPath: /home/fmriprep
        # Volume path to which we're linking (no starting '/')
        # Name the directory after the session so that the 
        # fmriprep directory on the container is always empty at the start
        subPath: '{{inputs.parameters.session}}'
      image: public.ecr.aws/docker/library/python:3-slim-bullseye
      imagePullPolicy: IfNotPresent
      resources:
        requests:
          ephemeral-storage: 50M
      command: [python]
      source: |
        from pathlib import Path
        import json
        tasks = list(set([file.name.split('_')[2].split('-')[1] for file in Path(f'/home/fmriprep/output/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func').glob('*')]))
        print(json.dumps(tasks))

  - name: functional-task-processing-template
    inputs:
      parameters:
        - name: session
        - name: task
    
    dag:
      tasks:
        # - name: rest-processing-dagtask
        #   template: rest-processing-template
        #   when: "{{inputs.parameters.task}} == rest"
        #   arguments:
        #     parameters:
        #       - name: session
        #         value: "{{inputs.parameters.session}}"
        #       - name: task
        #         value: "{{inputs.parameters.task}}"

        - name: nback-processing-dagtask
          template: nback-processing-template
          when: "{{inputs.parameters.task}} == nback"
          arguments:
            parameters:
              - name: session
                value: "{{inputs.parameters.session}}"
              - name: task
                value: "{{inputs.parameters.task}}"


  - name: rest-processing-template
    inputs:
      parameters:
        - name: session
        - name: task

    outputs:
      artifacts:
      - name: xcpd-output
        path: "/home/fmriprep/xcpd_output/{{workflow.parameters.subjID}}"
        s3:
          endpoint: s3-accelerate.amazonaws.com
          bucket: 'abcd-working'
          key: 'derivatives/xcpd/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/{{inputs.parameters.task}}-xcpd-output.tar.gz'
          region: us-east-2

    nodeSelector:
      karpenter.sh/nodepool: al2023-intel-heavy-nodepool
    
    container:
      securityContext:
        runAsUser: 0

      image: pennlinc/xcp_d:latest
      imagePullPolicy: IfNotPresent

      volumeMounts:
          # fmriprep base directory
          # PersistentVolumeClaim reference
        - name: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
          # Path inside the container where the volume will be mounted
          mountPath: /home/fmriprep
          # Volume path to which we're linking (no starting '/')
          # Name the directory after the session so that the 
          # fmriprep directory on the container is always empty at the start
          subPath: '{{inputs.parameters.session}}'

      resources:
        requests:
          memory: 16G
          cpu: '8'

      args:
        - '--mode'
        - 'none'
        - '--participant-label'
        - '{{workflow.parameters.ndaGUID}}'
        - '--session-id'
        - '{{inputs.parameters.session}}'
        - '--task-id'
        - '{{inputs.parameters.task}}'
        - '--nprocs'
        - '8'
        - '--input-type'
        - 'fmriprep'
        - '--file-format'
        - 'cifti'
        - '--dummy-scans' 
        - 'auto'
        - '--despike'
        - 'y'
        - '--nuisance-regressors'
        - '36P' 
        - '--smoothing'
        - '0' # see https://onlinelibrary.wiley.com/doi/10.1111/ejn.13717 which recommends against smoothing for graph theory analyses
        - '--combine-runs'
        - 'y'
        - '--motion-filter-type'
        - 'lp'
        - '--band-stop-min'
        - '12' # breaths per minute (0.2Hz) - lowest rate in range of values from 9-18 yrs (see: https://doi.org/10.1016/S0140-6736(10)62226-X)
        - '--motion-filter-order'
        - '4' # default when motion-filter-type is set to 'lp'
        - '--fd-thresh'
        - '0'
        - '--output-type'
        - 'censored'
        - '--lower-bpf'
        - '0.01' # default
        - '--upper-bpf'
        - '0.08' # default
        - '--atlases'
        - '4S556Parcels' # Need Schaefer 400 + Tian
        - 'Gordon'
        - '--min-coverage'
        - '0.5'
        # see: https://xcp-d.readthedocs.io/en/latest/workflows.html#abcd-mode
        #- '--create-matrices' # Creates matrices from a random subsample of volumes
        - '--random-seed'
        - '52618'
        - '--linc-qc'
        - 'y'
        - '--abcc-qc'
        - 'y'
        - '--report-output-level'
        - 'session'
        - '--clean-workdir'
        - '--notrack'
        - '--warp-surfaces-native2std'
        - 'y'
        - '/home/fmriprep/output/'
        - '/home/fmriprep/xcpd_output/'
        - 'participant'


  - name: nback-processing-template
    inputs:
      parameters:
        - name: session
        - name: task
      artifacts:
        - name: nback-timing-run1
          path: "/home/fmriprep/output/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/nback_event_timings/{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-nback_run-01_events.tsv"
          s3:
            key: 'inputs/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-nback_run-01_events.tsv'

        - name: nback-timing-run2
          path: "/home/fmriprep/output/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/nback_event_timings/{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-nback_run-02_events.tsv"
          s3:
            key: 'inputs/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-nback_run-02_events.tsv'
          optional: true

        - name: nback-timing-run3
          path: "/home/fmriprep/output/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/nback_event_timings/{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-nback_run-03_events.tsv"
          s3:
            key: 'inputs/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-nback_run-03_events.tsv'
          optional: true

        - name: nback-timing-run4
          path: "/home/fmriprep/output/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/nback_event_timings/{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-nback_run-04_events.tsv"
          s3:
            key: 'inputs/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-nback_run-04_events.tsv'
          optional: true

        - name: nback_custom_confounds_config
          path: "/home/fmriprep/custom_confounds/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/{{inputs.parameters.task}}_custom_confounds_config.yaml"
          s3:
            key: "config/nback_custom_confounds_config.yaml"

        - name: dataset-description
          path: "/home/fmriprep/output/dataset_description.json"
          s3:
            key: "config/dataset_description.json"

    outputs:
      artifacts:
      - name: xcpd-output
        path: "/home/fmriprep/xcpd_output/{{workflow.parameters.subjID}}"
        s3:
          endpoint: s3-accelerate.amazonaws.com
          bucket: 'abcd-working'
          key: 'derivatives/xcpd/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/{{inputs.parameters.task}}-xcpd-output.tar.gz'
          region: us-east-2

    nodeSelector:
      karpenter.sh/nodepool: al2023-intel-heavy-nodepool

    initContainers:

      - name: generating-task-regressors
        mirrorVolumeMounts: true
        image: public.ecr.aws/l9e7l1h1/cloudpipe/bravepy:latest
        imagePullPolicy: IfNotPresent
        command: ["python", "-c"]
        args: 
          - |
            import numpy
            import json
            import os
            import sys
            import numpy as np
            import pandas as pd
            from pathlib import Path
            import nibabel as nib
            from nilearn.glm.first_level import make_first_level_design_matrix

            TR = 0.8

            confounds_root = Path("/home/fmriprep/custom_confounds/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/")
            func_root = Path('/home/fmriprep/output/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func')
            event_timings_path = func_root / "{{inputs.parameters.task}}_event_timings"
            event_timing_files = sorted(list([file for file in event_timings_path.glob('*_task-{{inputs.parameters.task}}_run-0*_events.tsv')]))
            niis = sorted(list([niigz for niigz in func_root.glob('*task-{{inputs.parameters.task}}_run-0*_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz')]))

            if len(event_timing_files) != len(niis):
              print('The number of event timing files for {{inputs.parameters.task}} does not match the number of NIFTI files. Exiting with error.')
              sys.exit(1)
            else:
              n_runs = len(niis)
              
            for i in range(1, n_runs + 1):
              run_base = '{{workflow.parameters.subjID}}_{{inputs.parameters.session}}_task-{{inputs.parameters.task}}'
              print(run_base)
              run_niigz = func_root / "_".join([run_base, f"run-0{i}", "space-MNI152NLin2009cAsym_desc-preproc", "bold.nii.gz"])
              print(run_niigz)
              run_img = nib.load(run_niigz)
              run_nvols = run_img.shape[3]
              run_frame_times = np.arange(run_nvols) * TR

              run_etf = func_root / "nback_event_timings" / "_".join([run_base, f"run-0{i}", "events.tsv"])
              print(run_etf)
              run_etf_df = pd.read_csv(run_etf, sep='\t', usecols=range(0, 3))
              
              run_task_confounds = make_first_level_design_matrix(
                run_frame_times,
                run_etf_df,
                hrf_model="spm",
                drift_model=None,
                add_regs=None
              )

              run_task_confounds = run_task_confounds.drop(columns='constant')
              run_task_confounds.to_csv(Path(confounds_root / "_".join([run_base, f"run-0{i}", "desc-confounds", "timeseries.tsv"])))

            # Include a dataset_description.json file
            with open("/home/fmriprep/custom_confounds/dataset_description.json", "w") as fo:
              json.dump(
                {
                  "Name": "NBack Custom Confounds",
                  "BIDSVersion": "1.6.0",
                  "DatasetType": "derivative",
                  "GeneratedBy": [
                    {
                      "Name": "Cloudpipe"
                    }
                  ]
                },
                fo,
              )

    container:
      securityContext:
        runAsUser: 0

      image: pennlinc/xcp_d:latest
      imagePullPolicy: IfNotPresent

      volumeMounts:
          # fmriprep base directory
          # PersistentVolumeClaim reference
        - name: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
          # Path inside the container where the volume will be mounted
          mountPath: /home/fmriprep
          # Volume path to which we're linking (no starting '/')
          # Name the directory after the session so that the 
          # fmriprep directory on the container is always empty at the start
          subPath: '{{inputs.parameters.session}}'

      resources:
        requests:
          memory: 16G
          cpu: '8'
          # ephemeral-storage: 50G

      args:
        - '--mode'
        - 'none'
        - '--participant-label'
        - '{{workflow.parameters.ndaGUID}}'
        - '--session-id'
        - '{{inputs.parameters.session}}'
        - '--task-id'
        - '{{inputs.parameters.task}}'
        - '--nprocs'
        - '8'
        - '--input-type'
        - 'fmriprep'
        - '--file-format'
        - 'cifti'
        - '--dummy-scans'
        - 'auto'
        - '--despike'
        - 'y'
        - '--datasets'
        - 'nback_custom=/home/fmriprep/custom_confounds'
        - '--nuisance-regressors'
        - '/home/fmriprep/custom_confounds/{{workflow.parameters.subjID}}/{{inputs.parameters.session}}/func/{{inputs.parameters.task}}_custom_confounds_config.yaml'
        - '--smoothing'
        - '0' # see https://onlinelibrary.wiley.com/doi/10.1111/ejn.13717 which recommends against smoothing for graph theory analyses
        - '--combine-runs'
        - 'y'
        - '--motion-filter-type'
        - 'lp'
        - '--band-stop-min'
        - '12' # breaths per minute (0.2Hz) - lowest rate in range of values from 9-18 yrs (see: https://doi.org/10.1016/S0140-6736(10)62226-X)
        - '--motion-filter-order'
        - '4' # default when motion-filter-type is set to 'lp'
        - '--fd-thresh'
        - '0' # <= 0 disables fd thresholding # Does this still allow motion info to be used in nuisance regression???
        - '--output-type'
        - 'censored'
        - '--lower-bpf'
        - '0.01' # default
        - '--upper-bpf'
        - '0.08' # default
        - '--atlases'
        - '4S556Parcels' # Need Schaefer 400 + Tian
        - 'Gordon'
        - '--min-coverage'
        - '0.5'
        - '--random-seed'
        - '52618'
        - '--linc-qc'
        - 'y'
        - '--abcc-qc'
        - 'y'
        - '--report-output-level'
        - 'session'
        - '--clean-workdir'
        - '--notrack'
        - '--warp-surfaces-native2std'
        - 'y'
        - '/home/fmriprep/output/'
        - '/home/fmriprep/xcpd_output/'
        - 'participant'
