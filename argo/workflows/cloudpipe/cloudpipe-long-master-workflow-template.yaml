apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  namespace: argo-workflows
  name: cloudpipe-long-master-workflow-template
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::575108944090:role/argo-workflows-runner

spec:
  # Pre-specify metadata for all workflows run from this workflow
  workflowMetadata:
    annotations:
      workflows.argoproj.io/title: "sub-{{workflow.parameters.ndaGUID}}"
      karpenter.sh/do-not-disrupt: "true"

  tolerations:
    - key: argo.com/workloads
      value: "true"
      effect: NoSchedule

  podMetadata:
    labels:
      subjectid: "sub-{{workflow.parameters.ndaGUID}}"

  # Create the following persistent volume claim for this workflow
  volumeClaimTemplates:
    # Naming the volume claim after the subject ID (for tracking)
    - metadata:
        # "Name" should be used to reference this PVC further down the workflow
        name: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
        # Specify a string that will be appended to the string created by generateName in the workflow metadata
        # By introducing the random character sequence from the metadata, we create a unique PVC for each workflow
        # run, even if it's for the same subject ID. Neat!
        generateName: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
      spec:
        accessModes: [ "ReadWriteMany" ]
        storageClassName: efs-sc
        resources:
          requests:
          # see https://github.com/kubernetes-sigs/aws-fsx-openzfs-csi-driver/blob/main/examples/kubernetes/dynamic-provisioning/volume/claim.yaml
          # When dynamically provisioning FSx for OpenZFS volumes, the requested storage value is ignored.
          # Instead, the CSI driver uses the storageCapacityReservation and storageCapacityQuota parameters to
          # configure the storage properties of the volume. These values are defined in the storage class.
          # The value below MUST be explicitly set to 1Gi, otherwise the creation will fail.
            storage: 50Gi

  podGC:
    strategy: OnWorkflowCompletion

  # When to clean up persistent volume claims
  volumeClaimGC:
    strategy: OnWorkflowCompletion

  serviceAccountName: argo-workflows-runner

  # Time to live after workflow is completed, replaces ttlSecondsAfterFinished
  # ttlStrategy:
  #   secondsAfterCompletion: 60

  podDisruptionBudget:
    minAvailable: "100%"

  # MUST include a global security context to ensure artifacts are passed with the same (accessible)
  # permissions or else some tasks may not be able to access passed artifacts (this took too long to figure out)
  # see: https://github.com/argoproj/argo-workflows/issues/9651 (can also set 'mode: 755' e.g., to define permissions)
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  arguments:
    parameters:
      # Workflow parameters passed from command-line (-p) are indicated by lack of an accompanying value
      - name: ndaGUID
      # - name: miNDARpkgID
      # Creating new workflow parameters by modifying existing ones (using expr language)
      - name: subjID
        # Prepending the NDA GUID with 'sub-' per BIDS format
        value: sub-{{workflow.parameters.ndaGUID}}

  # Which template (below) should we start with?
  entrypoint: master-pipeline-dag

  templates:
  # This is the master DAG for the entire pipeline
  - name: master-pipeline-dag
    # Maximum number of pods (job steps) that can run at any one time
    parallelism: 3
    dag:
      # Do NOT stop the workflow if one part of the DAG fails
      # Necessary for subject_sessions that may have only anatomicals, but no functionals
      failFast: false
      # Providing each task in the DAG, its dependencies, and any output artifacts to be passed
      tasks:
        # What we'll call this task within this document
        - name: subject-data-inventory-dagtask
          # Reference to a template contained in a separate file
          templateRef:
            # The metadata name of the workflow template
            name: subject-data-inventory-workflow-template
            # The task template within the larger workflow template to be used as the 'entrypoint'
            template: subject-data-inventory-template

        # Anatomical processing pipeline
        - name: anatomical-processing-pipeline-dagtask
          # Reference to an in-line DAG template (provided below)
          template: anatomical-processing-pipeline-dag-template
          depends: "subject-data-inventory-dagtask.Succeeded"
          arguments:
            parameters:
              # Receiving the output from the subject-data-inventory-dagtask as a parameter named T1w_sessions
              - name: T1w_sessions
                value: '{{tasks.subject-data-inventory-dagtask.outputs.result}}'

        - name: functional-processing-pipeline-dagtask
          template: functional-processing-pipeline-dag-template
          depends: "anatomical-processing-pipeline-dagtask.Succeeded"
          arguments:
            parameters:
              - name: session
                value: '{{item}}'
          withParam: '{{tasks.subject-data-inventory-dagtask.outputs.result}}'



#######################################
#----- Anatomical Processing DAG -----#
#######################################

  - name: anatomical-processing-pipeline-dag-template
    # Maximum number of steps in this workflow that can run concurrently (DAG width)
    # Set to 2 per long_fastsurfer_run.sh
    parallelism: 1
    inputs:
      parameters:
        # Pull in in the T1w_sessions parameter when calling the DAG template above
        - name: T1w_sessions
    dag:
      tasks:
        # Create a within-subject template and register all the scans to it
        - name: fastsurfer-template-creation-dagtask
          templateRef:
            name: fastsurfer-template-creation-workflow-template
            template: fastsurfer-template-creation-dag-template
          arguments:
            parameters:
              # Pull in the T1w_sessions parameter passed in to the in-line DAG template (above)
              - name: T1w_sessions
                value: "{{inputs.parameters.T1w_sessions}}"

        # Segment the within-subject template
        - name: fastsurfer-template-segmentation-dagtask
          templateRef:
            name: fastsurfer-template-segmentation-workflow-template
            template: fastsurfer-template-segmentation-template
          # Depends on creation of the within-subject template
          dependencies:
            - fastsurfer-template-creation-dagtask

        # Parcellate the within-subject template
        - name: fastsurfer-template-parcellation-steps
          templateRef:
            name: fastsurfer-template-parcellation-workflow-template
            template: fastsurfer-template-parcellation-steps-template
          # Template segmentation is a pre-requisite for parcellation
          dependencies:
            - fastsurfer-template-segmentation-dagtask

        # Segment each of the newly-registered session scans
        # Can be run concurrent to within-subject seg/parc
        - name: fastsurfer-long-segmentation-steps
          templateRef:
            name: fastsurfer-long-segmentation-workflow-template
            template: fastsurfer-long-segmentation-steps-template
          dependencies:
            - fastsurfer-template-parcellation-steps
          arguments:
            parameters:
              - name: T1w_sessions
                value: "{{inputs.parameters.T1w_sessions}}"

        # Parcellate each of the registered session scans
        - name: fastsurfer-long-parcellation-steps
          templateRef:
            name: fastsurfer-long-parcellation-workflow-template
            template: fastsurfer-long-parcellation-steps-template
          # Requires all previous steps
          dependencies:
            - fastsurfer-long-segmentation-steps
          arguments:
            parameters:
              - name: T1w_sessions
                value: "{{inputs.parameters.T1w_sessions}}"


#######################################
#----- Functional Processing DAG -----#
#######################################

  - name: functional-processing-pipeline-dag-template
    inputs:
      parameters:
        - name: session

    dag:
      tasks:
        # Check if the session for this subject has fMRI scans
        # Should rewrite this to produce a list of tasks names (instead of true/false)
        # so we can pass it to xcpd-dagtask and eliminate the separate task inventory
        # script in that.

        - name: functional-preprocessing-dagtask
          templateRef:
            name: functional-preprocessing-workflow-template
            template: functional-preprocessing-steps-template
          arguments:
            parameters:
              - name: session
                value: '{{inputs.parameters.session}}'

        # - name: functional-postprocessing-dagtask
        #   templateRef:
        #     name: functional-postprocessing-workflow-template
        #     template: functional-postprocessing-dag-template
        #   depends: "functional-preprocessing-dagtask.Succeeded"
        #   arguments:
        #     parameters:
        #       - name: session
        #         value: '{{inputs.parameters.session}}'
