apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  namespace: argo-workflows
  name: cloudpipe-long-master-workflow-template
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::575108944090:role/argo-workflows-runner

spec:
  # Pre-specify metadata for all workflows run from this workflow
  workflowMetadata:
    annotations:
      workflows.argoproj.io/title: "sub-{{workflow.parameters.ndaGUID}}"
    labelsFrom:
      subjectID: 
        expression: workflow.parameters.ndaGUID
  podMetadata:
    labels:
      subjectid: "sub-{{workflow.parameters.ndaGUID}}"

  # Create the following persistent volume claim for this workflow
  volumeClaimTemplates:
    # Naming the volume claim after the subject ID (for tracking)
    - metadata:
        # "Name" should be used to reference this PVC further down the workflow
        name: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
        # Specify a string that will be appended to the string created by generateName in the workflow metadata
        # By introducing the random character sequence from the metadata, we create a unique PVC for each workflow
        # run, even if it's for the same subject ID. Neat!
        generateName: "sub-{{=lower(workflow.parameters.ndaGUID)}}"
      spec:
        accessModes:
          - ReadWriteMany
        storageClassName: efs-sc
        resources:
          requests:
            storage: 30Gi

  # When to clean up persistent volume claims
  volumeClaimGC: 
    strategy: OnWorkflowCompletion

  serviceAccountName: argo-workflows-runner

  # Time to live after workflow is completed, replaces ttlSecondsAfterFinished
  ttlStrategy:
    secondsAfterCompletion: 60 

  # When should an individual workflow step be retried and how many times max?
  retryStrategy:
    retryPolicy: OnError
    limit: '1'

  podDisruptionBudget:
    minAvailable: 9999

  # MUST include a global security context to ensure artifacts are passed with the same (accessible) 
  # permissions or else some tasks may not be able to access passed artifacts (this took too long to figure out)
  # see: https://github.com/argoproj/argo-workflows/issues/9651 (can also set 'mode: 755' e.g., to define permissions)
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  arguments:
    parameters:
      # Workflow parameters passed from command-line (-p) are indicated by lack of an accompanying value
      - name: ndaGUID
      # - name: miNDARpkgID
      # Creating new workflow parameters by modifying existing ones (using expr language)
      - name: subjID
        # Prepending the NDA GUID with 'sub-' per BIDS format
        value: sub-{{workflow.parameters.ndaGUID}}

  # Which template (below) should we start with?
  entrypoint: master-pipeline-dag

  templates:
  # This is the master DAG for the entire pipeline
  - name: master-pipeline-dag
    # Limit to the number of pods (job steps) that can run at any one time
    parallelism: 1
    dag:
      # Providing each task in the DAG, its dependencies, and any output artifacts to be passed
      tasks:
        # What we'll call this task within this document
        - name: subject-data-inventory-dagtask
          # Reference to a template contained in a separate file
          templateRef:
            # The metadata name of the workflow template
            name: subject-data-inventory-workflow-template
            # The task template within the larger workflow template to be used as the 'entrypoint'
            template: subject-data-inventory-template

        # Anatomical processing pipeline
        - name: anatomical-processing-pipeline-dagtask
          # Reference to an in-line DAG template (provided below)
          template: anatomical-processing-pipeline-dag-template
          depends: "subject-data-inventory-dagtask.Succeeded"
          arguments:
            parameters:
              # Receiving the output from the subject-data-inventory-dagtask as a parameter named T1w_sessions
              - name: T1w_sessions
                value: '{{tasks.subject-data-inventory-dagtask.outputs.result}}'

        # Functional processing pipeline
        # For each session with a T1, pull all the fmri scans and run them through fmriprep
        - name: functional-processing-pipeline-dagtask
          templateRef:
            name: fmriprep-workflow-template
            template: fmriprep-template
          depends: "anatomical-processing-pipeline-dagtask.Succeeded"

          arguments:
            parameters:
              - name: session
                value: '{{item}}'
          withParam: '{{tasks.subject-data-inventory-dagtask.outputs.result}}'

###################################
#----- In-line DAG Templates -----#
###################################

  - name: anatomical-processing-pipeline-dag-template
    # Maximum number of steps in this workflow that can run concurrently (DAG width)
    # Set to 2 per long_fastsurfer_run.sh
    parallelism: 1
    inputs:
      parameters:
        # Pull in in the T1w_sessions parameter when calling the DAG template above
        - name: T1w_sessions
    dag:
      tasks:
        # Create a within-subject template and register all the scans to it
        - name: fastsurfer-template-creation-dagtask
          templateRef:
            name: fastsurfer-template-creation-workflow-template
            template: fastsurfer-template-creation-dag-template
          arguments:
            parameters:
              # Pull in the T1w_sessions parameter passed in to the in-line DAG template (above)
              - name: T1w_sessions
                value: "{{inputs.parameters.T1w_sessions}}"

        # Segment the within-subject template
        - name: fastsurfer-template-segmentation-dagtask
          templateRef:
            name: fastsurfer-template-segmentation-workflow-template
            template: fastsurfer-template-segmentation-template
          # Depends on creation of the within-subject template
          dependencies:
            - fastsurfer-template-creation-dagtask

        # Parcellate the within-subject template
        - name: fastsurfer-template-parcellation-steps
          templateRef: 
            name: fastsurfer-template-parcellation-workflow-template
            template: fastsurfer-template-parcellation-steps-template
          # Template segmentation is a pre-requisite for parcellation
          dependencies:
            - fastsurfer-template-segmentation-dagtask

        # Segment each of the newly-registered session scans
        # Can be run concurrent to within-subject seg/parc
        - name: fastsurfer-long-segmentation-steps
          templateRef: 
            name: fastsurfer-long-segmentation-workflow-template
            template: fastsurfer-long-segmentation-steps-template
          dependencies:
            - fastsurfer-template-parcellation-steps
          arguments:
            parameters:
              - name: T1w_sessions
                value: "{{inputs.parameters.T1w_sessions}}"

        # Parcellate each of the registered session scans
        - name: fastsurfer-long-parcellation-steps
          templateRef:
            name: fastsurfer-long-parcellation-workflow-template
            template: fastsurfer-long-parcellation-steps-template
          # Requires all previous steps
          dependencies:
            - fastsurfer-long-segmentation-steps
          arguments:
            parameters:
              - name: T1w_sessions
                value: "{{inputs.parameters.T1w_sessions}}"
