apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: fmriprep-test-workflow-template
  generateName: brave-pipeline-
  namespace: argo-workflows
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::026415314835:role/argo-workflows-s3access
spec:
  retryStrategy:
   retryPolicy: OnError
   limit: '1'
  workflowMetadata:
    annotations:
      workflows.argoproj.io/title: "sub-{{workflow.parameters.ndaGUID}}"
      workflows.argoproj.io/name: "sub-{{workflow.parameters.ndaGUID}}"
  serviceAccountName: argo-workflows-s3access
  entrypoint: master-pipeline-dag
  ttlStrategy:
    secondsAfterCompletion: 60 # Time to live after workflow is completed, replaces ttlSecondsAfterFinished

  volumeClaimGC: 
    strategy: OnWorkflowCompletion

  volumeClaimTemplates:
    - metadata:
        name: workdir
      spec:
        accessModes:
          - ReadWriteMany
        storageClassName: efs-sc
        resources:
          requests:
            storage: 30Gi

  # MUST include a global security context to ensure artifacts are passed with the same (accessible) 
  # permissions or else some tasks may not be able to access passed artifacts (this took too long to figure out)
  # see: https://github.com/argoproj/argo-workflows/issues/9651 (can also set 'mode: 755' e.g., to define permissions)
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
    runAsNonRoot: true

  # Arguments passed at the command line ('-p arg=')
  arguments:
    parameters:
      # Workflow parameters to be passed in as arguments to the workflow 
      # (e.g., from the command line) as signified by the lack of value
      - name: ndaGUID
      - name: miNDARpkgID
      # Creating new workflow parameters by modifying existing ones (using expr language)
      - name: subjID
        # Prepending the NDA GUID with 'sub-' per BIDS format
        value: sub-{{workflow.parameters.ndaGUID}}

  templates:
  # This is the master DAG for the entire pipeline
  - name: master-pipeline-dag
    # Limit to the number of pods (job steps) that can run at any one time
    parallelism: 2
    dag:
      # Providing each task in the DAG, its dependencies, and any output artifacts to be passed
      tasks:
        # What we'll call this task within this document
        - name: subject-data-inventory-dagtask
          # Reference to a template contained in a separate file
          templateRef:
            # The metadata name of the workflow template
            name: subject-data-inventory-workflow-template
            # The 'entrypoint' task template within the workflow template
            template: subject-data-inventory-template

        # For each session with a T1, pull all the fmri scans and run them through fmriprep
        # NOTE: This should be adjusted to pull only sessions with completed fastsurfer runs
        - name: functional-processing-pipeline-dagtask
          templateRef:
            name: fmriprep-workflow-template
            template: fmriprep-loop-template
          arguments:
            parameters:
              - name: T1w_sessions
                value: '{{tasks.subject-data-inventory-dagtask.outputs.result}}'
          dependencies:
            - subject-data-inventory-dagtask


