failOnInitError: "true"
deviceListStrategy: "envvar"
deviceIDStrategy: "uuid"

tolerations:
  # This toleration is deprecated. Kept here for backward compatibility
  # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
  - key: CriticalAddonsOnly
    operator: Exists
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
  - key: "batch.amazonaws.com/batch-node"
    operator: "Exists"
  
devicePlugin:
  enabled: true

gfd:
  enabled: true

nfd:
  nameOverride: node-feature-discovery
  enableNodeFeatureApi: true
  master:
    serviceAccount:
      name: node-feature-discovery
      create: true
    config:
      extraLabelNs: ["nvidia.com"]

  worker:
    tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: ""
      effect: "NoSchedule"
    - key: "nvidia.com/gpu"
      operator: "Equal"
      value: "present"
      effect: "NoSchedule"
    - key: "batch.amazonaws.com/batch-node"
      operator: "Exists"
    config:
      sources:
        pci:
          deviceClassWhitelist:
          - "02"
          - "03"
          deviceLabelFields:
          - vendor
config:
  map:
    default: |-
      version: v1
      flags:
        migStrategy: single
    # a100-40gb: |-
    #   version: v1
    #   flags:
    #     migStrategy: mixed
    #   sharing:
    #     timeSlicing:
    #       resources:
    #       - name: nvidia.com/mig-1g.10gb
    #         replicas: 4
    # tesla-t4: |-
    #   version: v1
    #   flags:
    #     migStrategy: none
    #   sharing:
    #     timeSlicing:
    #       resources:
    #       - name: nvidia.com/gpu
    #         replicas: 2
    # nvidia-l4: |-
    #   version: v1
    #   flags:
    #     migStrategy: none
    #   sharing:
    #     timeSlicing:
    #       resources:
    #       - name: nvidia.com/gpu
    #         replicas: 2


